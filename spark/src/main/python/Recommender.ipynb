{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Building a recommender\n",
    "\n",
    "\n",
    "### Content-based filtering\n",
    "Use the content or attributes of an item/user\n",
    "\n",
    "### Collaborative filtering\n",
    "Generate estimated preferences of users for items with **which they have not yet interacted**.\n",
    "\n",
    "#### User-based approach\n",
    "* Use the **known preferences of other users** that exhibit similar behavior.\n",
    "* Select a set of similar users and compute some form of combined score based on the items they have shown a preference for.\n",
    "\n",
    "#### Item-based approach\n",
    "* Computes some measure of similarity between items based on the **existing user-item preferences**.\n",
    "* Items that tend to be rated the same by similar users will be classed as similar under this approach.\n",
    "\n",
    "### Matrix Factorization\n",
    "* Explicit - Preference is explicit. e.g. ratings\n",
    "* Implicit - Binary Matrix of watched/not watched, Count Matrix of how many times you have watched.\n",
    "\n",
    "**ALTERNATING LEAST SQUARES** - ALS works by iteratively solving a series of least squares regression problems. \n",
    "* In each iteration,user- or item-factor matrices is treated as fixed, while the other one is updated using the fixed factor and the rating data.\n",
    "* Then, the factor matrix that was solved for is, in turn, treated as fixed, while the other one is updated.\n",
    "* This process continues until the model has converged (or for a fixed number of iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR = '../resources/data/ml-100k'\n",
    "user_path = os.path.abspath(DATA_DIR + '/u.user')\n",
    "item_path = os.path.abspath(DATA_DIR + '/u.item')\n",
    "rating_path = os.path.abspath(DATA_DIR + '/u.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DoubleType, TimestampType\n",
    "user_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"occupation\", StringType(), True),\n",
    "    StructField(\"zipCode\", StringType(), True)])\n",
    "\n",
    "item_schema = StructType([\n",
    "    StructField(\"itemId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"releaseDate\", StringType(), True),\n",
    "    StructField(\"videoReleaseDate\", StringType(), True),\n",
    "    StructField(\"imbdUrl\", StringType(), True),\n",
    "    StructField(\"unknown\", IntegerType(), True),\n",
    "    StructField(\"action\", IntegerType(), True),\n",
    "    StructField(\"adventure\", IntegerType(), True),\n",
    "    StructField(\"animation\", IntegerType(), True),\n",
    "    StructField(\"children\", IntegerType(), True),\n",
    "    StructField(\"comedy\", IntegerType(), True),\n",
    "    StructField(\"crime\", IntegerType(), True),\n",
    "    StructField(\"documentary\", IntegerType(), True),\n",
    "    StructField(\"drama\", IntegerType(), True),\n",
    "    StructField(\"fantasy\", IntegerType(), True),\n",
    "    StructField(\"noir\", IntegerType(), True),\n",
    "    StructField(\"horror\", IntegerType(), True),\n",
    "    StructField(\"musical\", IntegerType(), True),\n",
    "    StructField(\"mystery\", IntegerType(), True),\n",
    "    StructField(\"romance\", IntegerType(), True),\n",
    "    StructField(\"sciFi\", IntegerType(), True),\n",
    "    StructField(\"thriller\", IntegerType(), True),\n",
    "    StructField(\"war\", IntegerType(), True),\n",
    "    StructField(\"western\", StringType(), True)])\n",
    "\n",
    "rating_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"itemId\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "userDf = spark.read.option(\"delimiter\",\"|\").csv('file://' + user_path, schema = user_schema)\n",
    "itemDf = spark.read.option(\"delimiter\",\"|\").csv('file://' + item_path, schema = item_schema)\n",
    "ratingDf = spark.read.option(\"delimiter\",\"\\t\").csv('file://' + rating_path, schema = rating_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-based model (Alternating Least Square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD using pyspark.mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "ratings = ratingDf.rdd.map(lambda l: Rating(l.userId, l.itemId, l.rating))\n",
    "\n",
    "# rank: Number of factors (hidden features) in our ALS model. (10 - 200 is usually reasonable)\n",
    "model_10 = ALS.train(ratings, rank=10, iterations=10, lambda_=0.01)\n",
    "model_50 = ALS.train(ratings, rank=50, iterations=10, lambda_=0.01, seed = 322)\n",
    "model_100 = ALS.train(ratings, rank=100, iterations=10, lambda_=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Features:  943\n",
      "Product Features:  1682\n"
     ]
    }
   ],
   "source": [
    "# We will have userFeature for each user and productFeature for each item - with the specified rank\n",
    "print('User Features: ' , model_10.userFeatures().count())\n",
    "print('Product Features: ', model_10.productFeatures().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF using pyspark.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS as ml_als\n",
    "als = ml_als(rank=50, maxIter=10, userCol=\"userId\", itemCol=\"itemId\", ratingCol=\"rating\", regParam=0.01, seed = 322)\n",
    "model_50_ = als.fit(ratingDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  (Rating(user=3, product=697, rating=5.75608262976389),\n",
       "   Rating(user=3, product=659, rating=5.433312345498309),\n",
       "   Rating(user=3, product=315, rating=5.249307269844362),\n",
       "   Rating(user=3, product=529, rating=5.241794827864484),\n",
       "   Rating(user=3, product=638, rating=5.0284916010276)))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_rdd = model_50.recommendProductsForUsers(5)\n",
    "top5_rdd.filter(lambda x: x[0] == 3).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                        |\n",
      "+------+---------------------------------------------------------------------------------------+\n",
      "|3     |[[430, 5.8786845], [59, 5.7773647], [663, 5.621893], [444, 5.597992], [320, 5.0101237]]|\n",
      "+------+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_df = model_50_.recommendForAllUsers(5)\n",
    "top5_df.filter(top5_df.userId == 3).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on training data\n",
    "\n",
    "* **RMSE/MSE** (Root Mean Squared Error)\n",
    "    * direct measure of the reconstruction error of the user-item rating matrix, commonly used in explicit ratings\n",
    "    * minimized by the conditional **mean**, suitable for asymmetric distribution and an unbiased fit.\n",
    "* **MAE** (Mean Absolute Error)\n",
    "    * minimized by the conditional **median**. The fit will be closer to the median and biased.\n",
    "* **MAP** (Mean Average Precision at K)\n",
    "    * When the order of recommendation is important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 10 Mean Squared Error = 0.482593374846323\n",
      "Rank 50 Mean Squared Error = 0.083412172973507\n",
      "Rank 100 Mean Squared Error = 0.014943923943970767\n"
     ]
    }
   ],
   "source": [
    "testdata = ratingDf.rdd.map(lambda l: (l.userId, l.itemId))\n",
    "actual = ratingDf.rdd.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "predictions_10 = model_10.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "predictions_50 = model_50.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "predictions_100 = model_100.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "MSE_10 = predictions_10.join(actual).map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "MSE_50 = predictions_50.join(actual).map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "MSE_100 = predictions_100.join(actual).map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Rank 10 Mean Squared Error = \" + str(MSE_10))\n",
    "print(\"Rank 50 Mean Squared Error = \" + str(MSE_50))\n",
    "print(\"Rank 100 Mean Squared Error = \" + str(MSE_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to HDFS and reload the model\n",
    "# model_50.save(sc, \"target/movielens-recommender\")\n",
    "# model = MatrixFactorizationModel.load(sc, \"target/movielens-recommender\")\n",
    "model = model_50\n",
    "predictions = predictions_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  = 0.083412172973507\n",
      "RMSE = 0.28881165657484636\n",
      "MAE  = 0.2034041795446187\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "joined_50 = predictions.join(actual)\n",
    "MSE_50 = joined_50.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "MAE_50 = joined_50.map(lambda r: math.fabs(r[1][0] - r[1][1])).mean()\n",
    "print(\"MSE  = \" + str(MSE_50))\n",
    "print(\"RMSE = \" + str(math.sqrt(MSE_50)))\n",
    "print(\"MAE  = \" + str(MAE_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  = 0.08341217297350699\n",
      "RMSE = 0.28881165657484636\n",
      "MAE  = 0.20340417954461876\n",
      "R-squared = 0.9341722794124651\n",
      "Explained variance = 1.0831797853882736\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "predictedAndActual = joined_50.map(lambda l: l[1])\n",
    "regressionMetrics = RegressionMetrics(predictedAndActual)\n",
    "\n",
    "# Squared Error\n",
    "print(\"MSE  = %s\" % regressionMetrics.meanSquaredError)\n",
    "print(\"RMSE = %s\" % regressionMetrics.rootMeanSquaredError)\n",
    "\n",
    "# Mean absolute error\n",
    "print(\"MAE  = %s\" % regressionMetrics.meanAbsoluteError)\n",
    "\n",
    "# R-squared\n",
    "print(\"R-squared = %s\" % regressionMetrics.r2)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained variance = %s\" % regressionMetrics.explainedVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the predictions for all users and items\n",
    "# +1 to user and item indices because they start from 1\n",
    "import numpy as np\n",
    "itemMatrix = np.array(model.productFeatures().map(lambda l: l[1]).collect())\n",
    "userMatrix = np.array(model.userFeatures().map(lambda l: l[1]).collect())\n",
    "all_predictions = []\n",
    "for idx, userVector in enumerate(userMatrix):\n",
    "    scores = itemMatrix.dot(userVector)\n",
    "    scores_with_idx = [(i+1,a) for i, a in enumerate(scores)]\n",
    "    scores_with_idx.sort(key=lambda tup: tup[1], reverse = True)\n",
    "    sorted_items = (idx+1, [tup[0] for tup in scores_with_idx])\n",
    "    all_predictions.append(sorted_items)\n",
    "predictAllDf = sc.parallelize(all_predictions).toDF() \\\n",
    "                .withColumnRenamed('_1', 'userId').withColumnRenamed('_2', 'predictedAll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Measures\n",
    "\n",
    "### Average Precision at K (apk/ ap@k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "# Taken and modified from https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "# Calculate the average precision of top k values\n",
    "def apk(actual, predicted, k=K):\n",
    "    \n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    \n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "        \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "    \n",
    "    #return num_hits / min(len(actual), k)  # if you return num_hits, you will get precision at K instead of apk\n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "\n",
    "apk_udf = f.udf(apk, DoubleType())\n",
    "\n",
    "# window by userId sorted by actual rating desc\n",
    "w1 = Window.partitionBy('userId').orderBy(f.desc('rating'))\n",
    "userMovieList = ratingDf.withColumn('items', f.collect_list('itemId').over(w1)) \\\n",
    "    .groupBy('userId').agg(f.max('items').alias('items'))\n",
    "\n",
    "predictionDF = predictions.map(lambda l: (l[0][0], l[0][1], l[1])).toDF()\n",
    "# window by userId sorted by predicted rating desc\n",
    "w2 = Window.partitionBy('_1').orderBy(f.desc('_3'))\n",
    "userPredictionList = predictionDF.withColumn('predicted', f.collect_list('_2').over(w2)) \\\n",
    "    .groupBy('_1').agg(f.max('predicted').alias('predicted')).withColumnRenamed('_1', 'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "apkDf = predictAllDf.join(userPredictionList, 'userId', 'left').join(userMovieList, 'userId', 'left') \\\n",
    "    .withColumn('items', f.when(f.col('items').isNull(), f.array().cast(\"array<integer>\")).otherwise(f.col('items'))) \\\n",
    "    .withColumn('predicted', f.when(f.col('predicted').isNull(), f.array().cast(\"array<integer>\")).otherwise(f.col('predicted'))) \\\n",
    "    .withColumn('apk', apk_udf(f.col('items'),f.col('predicted'))) \\\n",
    "    .withColumn('apkAll', apk_udf(f.col('items'),f.col('predictedAll')))   \n",
    "#apkDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@5 is 0.04058677978084126\n"
     ]
    }
   ],
   "source": [
    "mapk = apkDf.agg(f.avg('apkAll').alias('mapk')).collect()[0].mapk\n",
    "print('MAP@'+ str(K) +' is', mapk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP =  0.07205784911469854\n",
      "Precision@5 = 0.07126193001060449\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "rankingMetrics = RankingMetrics(apkDf.select('predictedAll', 'items').rdd.map(lambda l: (l.predictedAll, l.items)))\n",
    "print('MAP = ', rankingMetrics.meanAveragePrecision)\n",
    "print('Precision@' + str(K) + ' =', rankingMetrics.precisionAt(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating prediction for user 789, item 123:  3.1765442733287967\n"
     ]
    }
   ],
   "source": [
    "print('Rating prediction for user 789, item 123: ', model.predict(789, 123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations for user 789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Rating(user=789, product=504, rating=6.5757757828850725),\n",
       " Rating(user=789, product=715, rating=5.972789751703749),\n",
       " Rating(user=789, product=56, rating=5.821156452044317),\n",
       " Rating(user=789, product=47, rating=5.775194796607585),\n",
       " Rating(user=789, product=507, rating=5.539731363663269),\n",
       " Rating(user=789, product=101, rating=5.51104380813376),\n",
       " Rating(user=789, product=427, rating=5.369601850540164),\n",
       " Rating(user=789, product=772, rating=5.354445467654191),\n",
       " Rating(user=789, product=52, rating=5.345410382948737),\n",
       " Rating(user=789, product=357, rating=5.345010861865714)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top 10 recommendations for user 789')\n",
    "model.recommendProducts(789, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations for item 123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Rating(user=190, product=123, rating=6.105111706727907),\n",
       " Rating(user=246, product=123, rating=6.078838860749971),\n",
       " Rating(user=811, product=123, rating=5.698228719542813),\n",
       " Rating(user=548, product=123, rating=5.49832344438362),\n",
       " Rating(user=125, product=123, rating=5.284790748803018),\n",
       " Rating(user=597, product=123, rating=5.259023729879939),\n",
       " Rating(user=871, product=123, rating=5.0432371507945675),\n",
       " Rating(user=676, product=123, rating=5.023456831388421),\n",
       " Rating(user=103, product=123, rating=4.960695229655085),\n",
       " Rating(user=126, product=123, rating=4.953671109193628)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top 10 recommendations for item 123')\n",
    "model.recommendUsers(123, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-based (Cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect itemFactors to a dictionary\n",
    "itemFactors = model.productFeatures().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of 2 numpy arrays\n",
    "import numpy as np\n",
    "def cosineSimilarity(a, b):\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    return np.dot(a, b) / (norma * normb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor123 = itemFactors.get(123)\n",
    "npFactor123 = np.array(factor123)\n",
    "cosineSimilarity(npFactor123, npFactor123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemSimilarities = dict(map(lambda kv: (kv[0], cosineSimilarity(\n",
    "    np.array(kv[1]), npFactor123\n",
    ")), itemFactors.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(123, 1.0),\n",
       " (471, 0.8020833091931823),\n",
       " (436, 0.7948980428777909),\n",
       " (172, 0.7922805693228504),\n",
       " (568, 0.780453912924985),\n",
       " (50, 0.778950514377293),\n",
       " (1, 0.7778277588197464),\n",
       " (210, 0.7762099990810513),\n",
       " (433, 0.7758186194809127),\n",
       " (403, 0.7737553411333368)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 most similar items to item 123 (asc vs desc)\n",
    "sorted(itemSimilarities.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)[:10]\n",
    "#sorted(itemSimilarities.items(), key = lambda kv:(kv[1], kv[0]))[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
